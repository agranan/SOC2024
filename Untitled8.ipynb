{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1qY6fQMSHTdcgwTMboVKRybFm_1yjOm8Y",
      "authorship_tag": "ABX9TyP+yZFGBQnjAYco7aBfebpN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import glob\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import torchvision\n",
        "import argparse\n",
        "import yaml\n",
        "from torchvision.utils import make_grid\n"
      ],
      "metadata": {
        "id": "QTBMBDhjChdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9_qP8kAP7SJT"
      },
      "outputs": [],
      "source": [
        "class LinearNoiseScheduler:\n",
        "    def __init__(self, num_timesteps, beta_start, beta_end):\n",
        "        self.num_timesteps = num_timesteps\n",
        "        self.beta_start = beta_start\n",
        "        self.beta_end = beta_end\n",
        "\n",
        "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
        "        self.alphas = 1. - self.betas\n",
        "        self.alpha_cum_prod = torch.cumprod(self.alphas, dim = 0)\n",
        "        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod)\n",
        "        self.one_minus_alpha_cum_prod = torch.sqrt(1. - self.alpha_cum_prod)\n",
        "\n",
        "    def add_noise(self, original, noise, t):\n",
        "        original_shape = original.shape\n",
        "        batch_size = original_shape[0]\n",
        "\n",
        "        sqrt_alpha_cum_prod = self.sqrt_alpha_cum_prod[t].reshape(batch_size)\n",
        "        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod[t].reshape(batch_size)\n",
        "\n",
        "        for _ in range(len(original_shape) - 1):\n",
        "            sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.unsqueeze(-1)\n",
        "            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n",
        "\n",
        "        return sqrt_alpha_cum_prod * original + sqrt_one_minus_alpha_cum_prod * noise\n",
        "\n",
        "    def sample_prev_timestep(self, xt, noise_pred, t):\n",
        "        x0 = (xt - (self.sqrt_one_minus_alpha_cum_prod[t] * noise_pred)) / self.sqrt_alpha_cum_prod[t]\n",
        "        x0 = torch.clamp(x0, -1., 1.)\n",
        "        mean = xt - ((self.betas[t] * noise_pred) / (self.sqrt_one_minus_alpha_cum_prod[t]))\n",
        "        mean = mean / torch.sqrt(self.alphas[t])\n",
        "\n",
        "        if t == 0:\n",
        "            return mean, x0\n",
        "        else:\n",
        "            variance = (1 - self.alpha_cum_prod[t - 1]) / (1. - self.alpha_cum_prod[t])\n",
        "            variance = variance * self.betas[t]\n",
        "            sigma = variance ** 0.5\n",
        "            z = torch.randn(xt.shape).to(xt.device)\n",
        "            return mean + sigma * z, x0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_time_embedding(time_steps, t_emb_dim):\n",
        "    factor = 10000 ** ((torch.arange(\n",
        "        start = 0. end = t_emb_dim // 2, device = time_steps.device) / (t_emb_dim // 2)\n",
        "    ))\n",
        "    t_emb = time_steps[:, None].repeat(1, t_emb_dim // 2) / factor\n",
        "    t_emb = torch.at([torch.sin(t_emb), torch.cos(t_emb)], dim = -1)\n",
        "    return t_emb\n"
      ],
      "metadata": {
        "id": "JC-xWYomCdLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, t_emb_dim, down_sample, num_heads):\n",
        "        super().__init__()\n",
        "        self.down_sample = down_sample\n",
        "        self.resnet_conv_first = nn.Sequential(\n",
        "            nn.GroupNorm(8, in_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n",
        "        )\n",
        "        self.t_emb_layers = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(t_emb_dim, out_channels)\n",
        "        )\n",
        "        self.resnet_conv_second = nn.Sequential(\n",
        "            nn.GroupNorm(8, out_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n",
        "        )\n",
        "        self.attention_norm = nn.GroupNorm(8, out_channels)\n",
        "        self.attention = nn.MultiheadAttention(out_channels, num_heads, batch_first = True)\n",
        "        self.residual_input_conv = nn.Conv2d(in_channels, out_channles, kernel_size = 4,\n",
        "                                             stride = 2, padding = 1) if self.down_sample else nn.Identity()\n",
        "\n",
        "    def forward(self, x, t_emb):\n",
        "        out = x;\n",
        "\n",
        "        resnet_input = out\n",
        "        out = self.resnet_conv_first(out)\n",
        "        out = out + self.t_emb_layers(t_emb)[:, :, None, None]\n",
        "        out= self.resnet_conv_second(out)\n",
        "        out = out + self.residual_input_conv(resnet_input)\n",
        "\n",
        "        batch_size, channels, h, w = out.shape\n",
        "        in_attn = out.reshape(batch_size, channels, h * w)\n",
        "        in_attn = self.attention_norm(in_attn)\n",
        "        in_attn = in_attn.transpose(1, 2)\n",
        "        out_attn, _ = self.attention(in_attn, in_attn, in_attn)\n",
        "        out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "        out = out + out_attn\n",
        "\n",
        "        out = self.down_sample_conv(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "QcEqU7GOPFxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MidBlock(nn.Module):\n",
        "     def __init__(self, in_channels, out_channels, t_emb_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.resnet_conv_first = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.GroupNorm(8, in_channels),\n",
        "                nn.SiLU(),\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.GroupNorm(8, out_channels),\n",
        "                nn.SiLU(),\n",
        "                nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n",
        "            )\n",
        "        ])\n",
        "        self.t_emb_layers = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(t_emb_dim, out_channels)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(t_emb_dim, out_channels)\n",
        "            )\n",
        "        ])\n",
        "        self.resnet_conv_second = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.GroupNorm(8, out_channels),\n",
        "                nn.SiLU(),\n",
        "                nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n",
        "            ),\n",
        "            nn.Sequential(\n",
        "                nn.GroupNorm(8, out_channels),\n",
        "                nn.SiLU(),\n",
        "                nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n",
        "            ),\n",
        "        ])\n",
        "        self.attention_norm = nn.GroupNorm(8, out_channels)\n",
        "        self.attention = nn.MultiheadAttention(out_channels, num_heads, batch_furst = True)\n",
        "        self.residual_input_conv = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size = 1),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size = 1)\n",
        "        ])\n",
        "\n",
        "     def forward(self, x, t_emb):\n",
        "        out = x\n",
        "\n",
        "        resnet_input = out\n",
        "        out = self.resnet_conv_first[0](out)\n",
        "        out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n",
        "        out = self.resnet_conv_second[0](out)\n",
        "        out = out + self.residual_input_conv[0](resnet_input)\n",
        "\n",
        "        batch_size, channels, h, w = out.shape\n",
        "        in_attn = out.reshape(batch_size, channels, h * w)\n",
        "        in_attn = self.attention_norm(in_attn)\n",
        "        in_attn = in_attn.transpose(1, 2)\n",
        "        out_attn, _ = self.attention(in_attn, in_attn, in_attn)\n",
        "        out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "        out = out + out_attn\n",
        "\n",
        "        resnet_input = out\n",
        "        out = self.resnet_conv_first[1](out)\n",
        "        out = out + self.t_emb_layers[1](t_emb)[:, :, None, None]\n",
        "        out = self.resnet_conv_second[1](out)\n",
        "        out = out + self.residual_input_conv[1](resnet_input)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "N6hOEqXQTcND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample, num_heads):\n",
        "        super().__init__()\n",
        "        self.up_smaple = up_sample\n",
        "        self.resnet_conv_first = nn.Sequential(\n",
        "            nn.GroupNorm(8, in_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n",
        "        )\n",
        "        self.t_emb_layers = nn.Sequential(\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(t_emb_dim, out_channels)\n",
        "        )\n",
        "        self.resnet_conv_second = nn.Sequential(\n",
        "            nn.GroupNorm(8, out_channels),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n",
        "        )\n",
        "\n",
        "        self.attention_norm = nn.GroupNorm(8, out_channels)\n",
        "        self.attention = nn.MultiheadAttention(out_channels, num_heads, batch_first = True)\n",
        "        self.residual_input_conv = nn.Conv2d(in_channels, out_channels, kernel_size = 1)\n",
        "        self.up_sample_conv = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, kernel_size = 4,\n",
        "                                                 stride = 2, padding = 1) if self.up_sample else nn.Identity()\n",
        "\n",
        "    def forward(self, x, t_emb):\n",
        "        x = self.up_sample_conv(x)\n",
        "        x = torch.cat([x, out_down], dim = 1)\n",
        "\n",
        "\n",
        "        out = x\n",
        "        resnet_input = out\n",
        "        out = self.resnet_conv_first(out)\n",
        "        out = out + self.t_emb_layers(t_emb)[:, :, None, None]\n",
        "        out = self.resnet_conv_second(out)\n",
        "        out = out + self.residual_input_conv(resnet_input)\n",
        "\n",
        "        batch_size, channels, h, w = out.shape\n",
        "        in_attn = out.reshape(batch_size, channels, h * w)\n",
        "        in_attn = self.attention_norm(in_attn)\n",
        "        in_attn = in_attn.transpose(1, 2)\n",
        "        out_attn, _ = self.attention(in_attn, in_attn, in_attn)\n",
        "        out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n",
        "        out = out + out_attn\n",
        "\n",
        "        return out\n",
        ""
      ],
      "metadata": {
        "id": "FmVINK6cWzzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Unet(nn.Module):\n",
        "    def __init__(self, im_channels):\n",
        "        super().__init__()\n",
        "        self.down_channels = [32, 64, 128, 256]\n",
        "        self.mid_channels = [256, 256, 128]\n",
        "        self.t_emb_dim = 128\n",
        "        self.down_sample = [True, True, False]\n",
        "\n",
        "        self.t_proj = nn.Sequential(\n",
        "            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n",
        "        )\n",
        "        self.up_sample = list(reversed(self.down_sample))\n",
        "        self.conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size = 3, padding = 1)\n",
        "\n",
        "        self.downs = nn.ModuleList([])\n",
        "        for i in range(len(self.down_channels) - 1):\n",
        "            self.downs.append(DownBlock(self.down_channels[i], self.down_channels[i + 1], self.t_emb_dim,\n",
        "                                        down_sample = self.down_sample[i], num_heads = 4))\n",
        "\n",
        "        self.mids = nn.ModuleList([])\n",
        "        for i in range(len(self.mid_channels) - 1):\n",
        "            self.mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i + 1], self.t_emb_dim, num_heads = 4))\n",
        "\n",
        "        self.ups = nn.ModuleList([])\n",
        "        for i in reversed(range(len(self.down_channels) - 1)):\n",
        "            self.ups.append(UpBlock(self.down_channels[i] * 2, self.down_channels[i - 1] if i!= 0 else 16,\n",
        "                                    self.t_emb_dim, up_sample = self.down_sample[i], num_heads = 4))\n",
        "\n",
        "        self.norm_out = nn.GroupNorm(8, 16)\n",
        "        self.conv_out = nn.Conv2d(16, im_channels, kernel_size = 3, padding = 1)\n",
        "\n",
        "    def foward(self, x, t):\n",
        "        out = self.conv_in(x)\n",
        "        t_emb = get_time_embedding(t, self.t_emb_dim)\n",
        "        t_emb = self.t_proj(t_emb)\n",
        "\n",
        "        down_outs = []\n",
        "        for down in self.downs:\n",
        "            print(out.shape)\n",
        "            down_outs.append(out)\n",
        "            out = down(out, t_emb)\n",
        "\n",
        "        for mid in self.mids:\n",
        "            print(out.shape)\n",
        "            out = mid(out, t_emb)\n",
        "\n",
        "        for up in self.ups:\n",
        "            down_out = down_outs.pop()\n",
        "            print(out.shape, down_out.shape)\n",
        "            out = up(out, down_out, t_emb)\n",
        "        out = self.norm_out(out)\n",
        "        out = nn.SiLU()(out)\n",
        "        out = self.conv_out(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "FVBbVZJxhEav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MnistDataset(Dataset):\n",
        "\n",
        "    def __init__(self, split, im_path, im_ext = 'png'):\n",
        "        self.split = split\n",
        "        self.im_ext = im_ext\n",
        "        self.images, self.labels = self.load_images(im_path)\n",
        "\n",
        "    def load_images(self, im_path):\n",
        "        assert os.path.exists(im_path), \"images path {} does not exist\".format(im_path)\n",
        "        ims = []\n",
        "        labels = []\n",
        "        for d_name in tqdm(os.listdir(im_path)):\n",
        "            for fname in glob.glob(os.path.join(im_path, d_name, '*.{}'.format(self.im_ext))):\n",
        "                ims.append(fname)\n",
        "                labels.append(int(d_name))\n",
        "        print('Found {} images for split {}'.format(len(ims), self.split))\n",
        "        return ims, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        im = Image.open(self.images[index])\n",
        "        im_tensor = torchvision.transforms.ToTensor()(im)\n",
        "\n",
        "        im_tensor = (2 * im_tensor) - 1\n",
        "        return im_tensor"
      ],
      "metadata": {
        "id": "0A8_Vx3pkf5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args):\n",
        "    with open(args.config_path, 'r') as file:\n",
        "        try:\n",
        "            config = yaml.safe_load(file)\n",
        "        except yaml.YAMLError as exc:\n",
        "            print(exc)\n",
        "    print(config)\n",
        "\n",
        "    diffusion_config = config['diffusion_params']\n",
        "    dataset_config = config['dataset_params']\n",
        "    model_config = config['model_params']\n",
        "    train_config = config['train_params']\n",
        "\n",
        "    scheduler = LinearNoiseScheduler(num_timesteps = diffusion_config['num_timesteps'],\n",
        "                                     beta_start = diffusion_config['beta_start'],\n",
        "                                     beta_end = diffusion_config['beta_end'])\n",
        "\n",
        "    mnist = MnistDataset('train', im_path = dataset_config['im_path'])\n",
        "    mnist_loader = DataLoader(mnist, batch_size = train_config['batch_size'], shuffle = True, num_workers = 4)\n",
        "\n",
        "    model = Unet(model_config).to(device)\n",
        "    model.train()\n",
        "\n",
        "    if not os.path.exists(train_config['task_name']):\n",
        "        os.mkdir(train_config['task_name'])\n",
        "\n",
        "    if os.path.exists(os.path.join(train_config['task_name'], train_config['ckpt_name'])):\n",
        "        print('Loading checkpoint as found one')\n",
        "        model.load_state_dict(torch.load(os.path.join(train_config['task_name'],\n",
        "                                                      train_config['ckpt_name']), map_location = device))\n",
        "\n",
        "    num_epochs = train_config['num_epochs']\n",
        "    optimizer = Adam(model.parameters(), lr = train_config['lr'])\n",
        "    criterion = torch.nn.MSELoss()\n",
        "\n",
        "    for epoch_idx in range(num_epochs):\n",
        "        losses = []\n",
        "        for im in tqdm(mnist_loader):\n",
        "            optimizer.zero_grad()\n",
        "            im = im.float().to(device)\n",
        "\n",
        "            noise = torch.randn_like(im).to(device)\n",
        "\n",
        "            t = torch.randint(0, diffusion_config['num_timesteps'], (im.shape[0],)).to(device)\n",
        "\n",
        "            noisy_im - scheduler.add_noise(im, noise, t)\n",
        "            noise_pred = model(noisy_im, t)\n",
        "\n",
        "            loss = criterion(noise_pred, noise)\n",
        "            losses.append(loss.item())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print('Finished epoch: {} | Loss : {:.4f}'.format(\n",
        "            epoch_idx + 1,\n",
        "            np.mean(losses),\n",
        "        ))\n",
        "        torch.save(model.state_dict(), os.path.join(train_config['task_name'],\n",
        "                                                    train_config['ckpt_name']))\n",
        "    print('Done Training...')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description = \"Arguments for ddpm training\")\n",
        "    parser.add_argument('--config', dest = 'config_path',\n",
        "                        default = 'config/default.yaml', type = str)\n",
        "    args = parser.parse_args()\n",
        "    train(args)"
      ],
      "metadata": {
        "id": "whKZvqffl0Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(model, scheduler, train_config, model_config, diffusion_config):\n",
        "    xt = torch.randn((train_config['num_samples'],\n",
        "                      model_config['im_channels'],\n",
        "                      model_config['im_size'],\n",
        "                      model_config['im_size'])).to(device)\n",
        "\n",
        "    for i in tqdm(reversed(rnage(diffusion_config['num_timesteps']))):\n",
        "        noise_pred = model(xt, torch.as_tensor(i).unsqueeze(0).to(device))\n",
        "\n",
        "        xt, x0_pred = scheduler.sample_prev_timestep(xt, noise_pred, torch.as_tensor(i).to(device))\n",
        "\n",
        "        ims = torch.clamp(xt, -1., 1.).detach().cpu()\n",
        "        ims = (ims + 1) / 2\n",
        "        grid = make_grid(ims, nrow = train_config['num_grid_rows'])\n",
        "        img = torchvision.transforms.ToPILImage()(grid)\n",
        "        if not os.path.exists(os.path.join(train_config['task_name'], 'samples')):\n",
        "            os.mkdir(os.path.join(train_config['task_name'], 'samples'))\n",
        "        img.save(os.path.join(train_config['task_name'], 'samples', 'x0_{}.png'.format(i)))\n",
        "        img.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "lUlg2s9UowyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def infer(args):\n",
        "    with open(args.config_path, 'r') as file:\n",
        "        try:\n",
        "            config = yaml.safe_load(file)\n",
        "        except yaml.YAMLError as exc:\n",
        "            print(exc)\n",
        "    print(config)\n",
        "\n",
        "    diffusion_config = config['diffusion_params']\n",
        "    model_config = config['model_params']\n",
        "    train_config = config['train_params']\n",
        "\n",
        "    model = Unet(model_config).to(device)\n",
        "    model.load_state_dict(torch.load(os.path.join(train_config['task_name'],\n",
        "                                                  train_config['ckpt_name']), map_location = device))\n",
        "    model.eval()\n",
        "\n",
        "    scheduler = LinearNoiseScheduler(num_timesteps = diffusion_config['num_timesteps'],\n",
        "                                     beta_start = diffusion_config['beta_start'],\n",
        "                                     beta_end = diffusion_config['beta_end'])\n",
        "    with torch.no_grad():\n",
        "        sample(model, scheduler, train_config, model_config, diffusion_config)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description = 'Arguments for ddpm image generation')\n",
        "    parser.add_argument('--config', dest = 'config_path',\n",
        "                        default = 'config/default.yaml', type = str)\n",
        "    args = parser.parse_args()\n",
        "    infer(args)"
      ],
      "metadata": {
        "id": "kfFqkG6KnrMg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}