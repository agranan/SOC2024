{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8912504,"sourceType":"datasetVersion","datasetId":5359226},{"sourceId":8913083,"sourceType":"datasetVersion","datasetId":5359390}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from __future__ import absolute_import\nfrom collections import namedtuple\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom torch.autograd import Variable\nimport numpy as np\nimport torch.nn\nimport torchvision\nimport glob\nimport os\nimport random\nfrom PIL import Image\nfrom torch.utils.data.dataset import Dataset","metadata":{"id":"IIz92PRRNmGV","execution":{"iopub.status.busy":"2024-07-16T10:44:35.325789Z","iopub.execute_input":"2024-07-16T10:44:35.326645Z","iopub.status.idle":"2024-07-16T10:44:40.427448Z","shell.execute_reply.started":"2024-07-16T10:44:35.326611Z","shell.execute_reply":"2024-07-16T10:44:40.426665Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","metadata":{"id":"skuSeC0kO5uJ","execution":{"iopub.status.busy":"2024-07-16T10:44:40.429987Z","iopub.execute_input":"2024-07-16T10:44:40.430386Z","iopub.status.idle":"2024-07-16T10:44:40.488826Z","shell.execute_reply.started":"2024-07-16T10:44:40.430360Z","shell.execute_reply":"2024-07-16T10:44:40.487758Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_latents(latent_path):\n    latent_maps = {}\n    for fname in glob.glob(os.path.join(latent_path, '*.pkl')):\n        s = pickle.load(open(fname, 'rb'))\n        for k, v in s.items():\n            latent_maps[k] = v[0]\n    return latent_maps","metadata":{"id":"yyagjC-6lVpY","execution":{"iopub.status.busy":"2024-07-16T10:44:40.489929Z","iopub.execute_input":"2024-07-16T10:44:40.490241Z","iopub.status.idle":"2024-07-16T10:44:40.500744Z","shell.execute_reply.started":"2024-07-16T10:44:40.490217Z","shell.execute_reply":"2024-07-16T10:44:40.499880Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### **Dataset**","metadata":{"id":"3xTD5nkj9oUj"}},{"cell_type":"code","source":"# class CelebDataset(Dataset):\n#     def __init__(self, split, im_path, im_size = 256, im_channels = 3, im_ext = 'jpg',\n#                  use_latents = False, latent_path = None, condition_config = None):\n#         self.split = split\n#         self.im_size = im_size\n#         self.im_channels = im_channels\n#         self.im_ext = im_ext\n#         self.im_path = im_path\n#         self.latent_maps = None\n#         self.use_latents = False\n\n#         self.condition_types = [] if condition_config is None else condition_config['condition_types']\n#         self.idx_to_cls_map = {}\n#         self.cls_to_idx_map = {}\n\n#         if 'image' in self.condition_types:\n#             self.mask_channels = condition_config['image_condition_config']['image_condition_input_channels']\n#             self.mask_h = condition_config['image_condition_config']['image_condition_h']\n#             self.mask_w = condition_config['image_condition_config']['image_condition_w']\n\n#         self.images, self.texts, self.masks = self.load_images(im_path)\n\n#         if use_latents and latent_path is not None:\n#             latent_maps = load_latents(latent_path)\n#             if len(latent_maps) == len(self.images):\n#                 self.use_latents = True\n#                 self.latent_maps = latent_maps\n#                 print('Found {} latents'.format(len(self.latent_maps)))\n#             else:\n#                 print('Latents not found')\n\n\n#     def load_images(self, im_path):\n#         assert os.path.exists(im_path), \"images path {} does not exist\".format(im_path)\n#         ims = []\n#         fnames = glob.glob(os.path.join(im_path, 'CelebA-HQ-img/*.{}'.format('png')))\n#         fnames += glob.glob(os.path.join(im_path, 'CelebA-HQ-img/*.{}'.format('jpg')))\n#         fnames += glob.glob(os.path.join(im_path, 'CelebA-HQ-img/*.{}'.format('jpeg')))\n#         texts = []\n#         masks = []\n#         if 'image' in self.condition_types:\n#             label_list = ['skin', 'nose', 'eye_g', 'l_eye', 'r_eye', 'l_brow', 'r_brow', 'l_ear', 'r_ear', 'mouth',\n#                           'u_lip', 'l_lip', 'hair', 'hat', 'ear_r', 'neck_l', 'neck', 'cloth']\n#             self.idx_to_cls_map = {idx: label_list[idx] for idx in range(len(label_list))}\n#             self.cls_to_idx_map = {label_list[idx]: idx for idx in range(len(label_list))}\n#         for fname in tqdm(fnames):\n#             ims.append(fname)\n\n#             if 'text' in self.condition_types:\n#                 im_name = os.path.split(fname)[1].split('.')[0]\n#                 captions_im = []\n#                 with open(os.path.join(im_path, 'celeba-caption/{}.txt'.format(im_name))) as f:\n#                     for line in f.readlines():\n#                         captions_im.append(line.strip())\n#                 texts.append(captions_im)\n\n#             if 'image' in self.condition_types:\n#                 im_name = int(os.path.split(fname)[1].split('.')[0])\n#                 masks.append(os.path.join(im_path, 'CelebAMask-HQ-mask', '{}.png'.format(im_name)))\n#         if 'text' in self.condition_types:\n#             assert len(texts) == len(ims), \"Condition Type Text but could not find captions for all images\"\n#         if 'image' in self.condition_types:\n#             assert len(masks) == len(ims), \"Condition Type Image but could not find masks for all images\"\n#         print('Found {} images'.format(len(ims)))\n#         print('Found {} masks'.format(len(masks)))\n#         print('Found {} captions'.format(len(texts)))\n#         return ims, texts, masks\n\n#     def get_mask(self, index):\n#         mask_im = Image.open(self.masks[index])\n#         mask_im = np.array(mask_im)\n#         im_base = np.zeros((self.mask_h, self.mask_w, self.mask_channels))\n#         for orig_idx in range(len(self.idx_to_cls_map)):\n#             im_base[mask_im == (orig_idx+1), orig_idx] = 1\n#         mask = torch.from_numpy(im_base).permute(2, 0, 1).float()\n#         return mask\n#     def __len__(self):\n#         return len(self.images)\n\n#     def __getitem__(self, index):\n#         cond_inputs = {}\n#         if 'text' in self.condition_types:\n#             cond_inputs['text'] = random.sample(self.texts[index], k=1)[0]\n#         if 'image' in self.condition_types:\n#             mask = self.get_mask(index)\n#             cond_inputs['image'] = mask\n#             if self.use_latents:\n#                 latent = self.latent_maps[self.images[index]]\n#             if len(self.condition_types) == 0:\n#                 return latent\n#             else:\n#                 return latent, cond_inputs\n#         else:\n#             im = Image.open(self.images[index])\n#             im_tensor = torchvision.transforms.Compose([\n#                 torchvision.transforms.Resize(self.im_size),\n#                 torchvision.transforms.CenterCrop(self.im_size),\n#                 torchvision.transforms.ToTensor(),\n#             ])(im)\n#             im.close()\n#             im_tensor = (2 * im_tensor) - 1\n#             if len(self.condition_types) == 0:\n#                 return im_tensor\n#             else:\n#                 return im_tensor, cond_inputs","metadata":{"id":"TykgwB9j9rFA","execution":{"iopub.status.busy":"2024-07-16T10:45:08.291152Z","iopub.execute_input":"2024-07-16T10:45:08.291539Z","iopub.status.idle":"2024-07-16T10:45:08.299448Z","shell.execute_reply.started":"2024-07-16T10:45:08.291510Z","shell.execute_reply":"2024-07-16T10:45:08.298492Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### **# Blocks**","metadata":{"id":"JVXJFawA5nQC"}},{"cell_type":"code","source":"def get_time_embedding(time_steps, temb_dim):\n    assert temb_dim % 2 == 0, 'time embedding dimension must be divisible by 2'\n    factor = 10000 ** ((torch.arange(\n        start = 0, end = temb_dim // 2, dtype = torch.float32, device = time_steps.device) / (temb_dim // 2))\n    )\n    t_emb = time_steps[:, None].repeat(1, temb_dim // 2) / factor\n    t_emb = torch.cat([torch.sin(t_emb), torch.cos(t_emb)], dim = -1)\n    return t_emb","metadata":{"id":"Un_EcXJCPJxo","execution":{"iopub.status.busy":"2024-07-16T10:45:11.640322Z","iopub.execute_input":"2024-07-16T10:45:11.641302Z","iopub.status.idle":"2024-07-16T10:45:11.646975Z","shell.execute_reply.started":"2024-07-16T10:45:11.641267Z","shell.execute_reply":"2024-07-16T10:45:11.646096Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class DownBlock(nn.Module):\n\n    def __init__(self, in_channels, out_channels, t_emb_dim, down_sample, num_heads, num_layers, attn, norm_channels, cross_attn = False, context_dim = None):\n        super().__init__()\n        self.num_layers = num_layers\n        self.down_sample = down_sample\n        self.attn = attn\n        self.context_dim = context_dim\n        self.cross_attn = cross_attn\n        self.t_emb_dim = t_emb_dim\n        self.resnet_conv_first = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n                )\n                for i in range(num_layers)\n            ]\n        )\n        if self.t_emb_dim is not None:\n            self.t_emb_layers = nn.ModuleList([\n                nn.Sequential(\n                    nn.SiLU(),\n                    nn.Linear(self.t_emb_dim, out_channels)\n                )\n                for _ in range(num_layers)\n            ])\n        self.resnet_conv_second = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n                )\n                for _ in range(num_layers)\n            ]\n        )\n        if self.attn:\n            self.attention_norms = nn.ModuleList(\n                [nn.GroupNorm(norm_channels, out_channels)\n                for _ in range(num_layers)]\n            )\n            self.attentions = nn.ModuleList(\n                [nn.MultiheadAttention(out_channels, num_heads, batch_first = True)\n                for _ in range(num_layers)]\n            )\n        if self.cross_attn:\n            assert context_dim is not None, \"Context dimension must be passed for cross attention\"\n            self.cross_attention_norms = nn.ModuleList(\n                [nn.GroupNorm(norm_channels, out_channels)\n                for _ in range(num_layers)]\n            )\n            self.cross_attentions = nn.ModuleList(\n                [nn.MultiheadAttention(out_channels, num_heads, batch_first = True)\n                for _ in range(num_layers)]\n            )\n            self.context_proj = nn.ModuleList(\n                [nn.Linear(context_dim, out_channels)\n                for _ in range(num_layers)]\n            )\n        self.residual_input_conv = nn.ModuleList(\n            [\n                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size = 1)\n                for i in range(num_layers)\n            ]\n        )\n        self.down_sample_conv = nn.Conv2d(out_channels, out_channels, 4, 2, 1) if self.down_sample else nn.Identity()\n\n    def forward(self, x, t_emb = None, context = None):\n        out = x\n#         print(out.size())\n        for i in range(self.num_layers):\n            resnet_input = out\n            out = self.resnet_conv_first[i](out)\n            if self.t_emb_dim is not None:\n                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n            out = self.resnet_conv_second[i](out)\n            out = out + self.residual_input_conv[i](resnet_input)\n\n            if self.attn:\n                batch_size, channels, h, w = out.shape\n                in_attn = out.reshape(batch_size, channles, h * w)\n                in_attn = self.attention_norms[i](in_attn)\n                in_attn = in_attn.transpose(1, 2)\n                out_attn, _ = self.attentions[i](in_attn , in_attn, in_attn)\n                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n                out = out + out_attn\n\n            if self.cross_attn:\n                assert context is not None, \"context cannot be none if cross attention layers are used\"\n                batch_size, channels, h, w = out.shape\n                in_attn = out.reshape(batch_size, channels, h * w)\n                in_attn = self.cross_attention_norms[i](in_attn)\n                in_attn = in_attn.transpose(1, 2)\n                assert contecxt.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim\n                context_proj = self.context_proj[i](context)\n                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n                out = out + out_attn\n        out = self.down_sample_conv(out)\n        return out\n\n","metadata":{"id":"sYr1eIPMQTQC","execution":{"iopub.status.busy":"2024-07-16T10:45:13.741865Z","iopub.execute_input":"2024-07-16T10:45:13.742202Z","iopub.status.idle":"2024-07-16T10:45:13.763654Z","shell.execute_reply.started":"2024-07-16T10:45:13.742177Z","shell.execute_reply":"2024-07-16T10:45:13.762665Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class MidBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, t_emb_dim, num_heads, num_layers, norm_channels, cross_attn = None, context_dim = None):\n        super().__init__()\n        self.num_layers = num_layers\n        self.t_emb_dim = t_emb_dim\n        self.context_dim = context_dim\n        self.cross_attn = cross_attn\n        self.resnet_conv_first = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n                )\n                for i in range(num_layers + 1)\n            ]\n        )\n        if self.t_emb_dim is not None:\n            self.t_emb_layers = nn.ModuleList([\n                nn.Sequential(\n                    nn.SiLU(),\n                    nn.Linear(t_emb_dim, out_channels)\n                )\n                for _ in range(num_layers + 1)\n            ])\n        self.resnet_conv_second = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n                )\n                for _ in range(num_layers + 1)\n            ]\n        )\n        self.attention_norms = nn.ModuleList(\n            [nn.GroupNorm(norm_channels, out_channels)\n            for _ in range(num_layers)]\n        )\n        self.attentions = nn.ModuleList(\n            [nn.MultiheadAttention(out_channels, num_heads, batch_first = True)\n            for _ in range(num_layers)]\n        )\n        if self.cross_attn:\n            assert context_dim is not None, \"Context dimension must be passed for cross attention\"\n            self.cross_attention_norms = nn.ModuleList(\n                [nn.GroupNorm(norm_channels, out_channels)\n                for _ in range(num_layers)]\n            )\n            self.cross_attentions = nn.ModuleList(\n                [nn.MultiheadAttention(out_channels, num_heads, batch_first = True)\n                for _ in range(num_layers)]\n            )\n            self.context_proj = nn.ModuleList(\n                [nn.Linear(context_dim, out_channels)\n                for _ in range(num_layers)]\n            )\n        self.residual_input_conv = nn.ModuleList(\n            [\n                nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size = 1)\n                for i in range(num_layers + 1)\n            ]\n        )\n\n    def forward(self, x, t_emb = None, context = None):\n        out = x\n        resnet_input = out\n        out = self.resnet_conv_first[0](out)\n        if self.t_emb_dim is not None:\n            out = out + self.t_emb_layers[0](t_emb)[:, :, None, None]\n        out = self.resnet_conv_second[0](out)\n        out = out + self.residual_input_conv[0](resnet_input)\n\n        for i in range(self.num_layers):\n            batch_size, channels, h, w = out.shape\n            in_attn = out.reshape(batch_size, channels, h * w)\n            in_attn = self.attention_norms[i](in_attn)\n            in_attn = in_attn.transpose(1, 2)\n            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n            out = out + out_attn\n\n            if self.cross_attn:\n                assert context is not None, \"context cannot be none if cross attention layers are used\"\n                batch_size, channels, h, w = out.shape\n                in_attn = out.reshape(batch_size, channels, h * w)\n                in_attn = self.cross_attention_norms[i](in_attn)\n                in_attn = in_attn.transpose(1, 2)\n                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim\n                context_proj = self.context_proj[i](context)\n                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n                out = out + out_attn\n\n            resnet_input = out\n            out = self.resnet_conv_first[i + 1](out)\n            if self.t_emb_dim is not None:\n                out = out + self.t_emb_layers[i + 1](t_emb)[:, :, None, None]\n            out = self.resnet_conv_second[i + 1](out)\n            out = out + self.residual_input_conv[i + 1](resnet_input)\n\n        return out\n\n","metadata":{"id":"yjs-2loKWzZz","execution":{"iopub.status.busy":"2024-07-16T10:45:17.217437Z","iopub.execute_input":"2024-07-16T10:45:17.217805Z","iopub.status.idle":"2024-07-16T10:45:17.243772Z","shell.execute_reply.started":"2024-07-16T10:45:17.217776Z","shell.execute_reply":"2024-07-16T10:45:17.242800Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class UpBlock(nn.Module):\n\n    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample, num_heads, num_layers, attn, norm_channels):\n        super().__init__()\n        self.num_layers = num_layers\n        self.up_sample = up_sample\n        self.t_emb_dim = t_emb_dim\n        self.attn = attn\n        self.resnet_conv_first = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n                )\n                for i in range(num_layers)\n            ]\n        )\n        if self.t_emb_dim is not None:\n            self.t_emb_layers = nn.ModuleList([\n                nn.Sequential(\n                    nn.SiLU(),\n                    nn.Linear(t_emb_dim, out_channels)\n                )\n                for _ in range(num_layers)\n            ])\n        self.resnet_conv_second = nn.ModuleList([\n            nn.Sequential(\n                nn.GroupNorm(norm_channels, out_channels),\n                nn.SiLU(),\n                nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1)\n            )\n            for _ in range(num_layers)\n        ])\n        if self.attn:\n            self.attention_norms = nn.ModuleList(\n                [nn.GroupNorm(norm_channels, out_channels)\n                for _ in range(num_layers)]\n            )\n            self.attentions = nn.ModuleList(\n                [nn.MultiheadAttention(out_channels, num_heads, batch_first = True)\n                for _ in range(num_layers)]\n            )\n        self.residual_input_conv = nn.ModuleList(\n            [nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size = 1)\n            for i in range(num_layers)]\n        )\n        self.up_sample_conv = nn.ConvTranspose2d(in_channels, in_channels, 4, 2, 1) if self.up_sample else nn.Identity()\n\n    def forward(self, x, out_down = None, t_emb = None):\n        x = self.up_sample_conv(x)\n        if out_down is not None:\n            x = torch.cat([x, out_down], dim = 1)\n        out = x\n        for i in range(self.num_layers):\n            resnet_input = out\n            out = self.resnet_conv_first[i](out)\n            if self.t_emb_dim is not None:\n                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n            out = self.resnet_conv_second[i](out)\n            out = out + self.residual_input_conv[i](resnet_input)\n\n            if self.attn:\n                batch_size, channels, h, w = out.shape\n                in_attn = out.reshape(batch_size, channels, h * w)\n                in_attn = self.attention_norms[i](in_attn)\n                in_attn = in_attn.transpose(1, 2)\n                out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n                out_attn = out_attn.tranpose(1, 2).reshape(batch_size, channels, h, w)\n                out = out + out_attn\n        return out\n\n","metadata":{"id":"S7ornCCStdLk","execution":{"iopub.status.busy":"2024-07-16T10:45:21.215339Z","iopub.execute_input":"2024-07-16T10:45:21.216284Z","iopub.status.idle":"2024-07-16T10:45:21.232642Z","shell.execute_reply.started":"2024-07-16T10:45:21.216233Z","shell.execute_reply":"2024-07-16T10:45:21.231589Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class UpBlockUnet(nn.Module):\n\n    def __init__(self, in_channels, out_channels, t_emb_dim, up_sample, num_heads, num_layers, attn, norm_channels):\n        super().__init__()\n        self.num_layers = num_layers\n        self.up_sample = up_sample\n        self.t_emb_dim = t_emb_dim\n        self.attn = attn\n        self.resnet_conv_first = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, in_channels if i == 0 else out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n                )\n                for i in range(num_layers)\n            ]\n        )\n        if self.t_emb_dim is not None:\n            self.t_emb_layers = nn.ModuleList([\n                nn.Sequential(\n                    nn.SiLU(),\n                    nn.Linear(t_emb_dim, out_channels)\n                )\n                for _ in range(num_layers)\n            ])\n\n        self.resnet_conv_second = nn.ModuleList(\n            [\n                nn.Sequential(\n                    nn.GroupNorm(norm_channels, out_channels),\n                    nn.SiLU(),\n                    nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n                )\n                for _ in range(num_layers)\n            ]\n        )\n        if self.attn:\n            self.attention_norms = nn.ModuleList(\n                [\n                    nn.GroupNorm(norm_channels, out_channels)\n                    for _ in range(num_layers)\n                ]\n            )\n\n            self.attentions = nn.ModuleList(\n                [\n                    nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n                    for _ in range(num_layers)\n                ]\n            )\n\n#         if self.cross_attn:\n#             assert context_dim is not None, \"Context Dimension must be passed for cross attention\"\n#             self.cross_attention_norms = nn.ModuleList(\n#                 [nn.GroupNorm(norm_channels, out_channels)\n#                 for _ in range(num_layers)]\n#             )\n#             self.cross_attentions = nn.ModuleList(\n#                 [nn.MultiheadAttention(out_channels, num_heads, batch_first=True)\n#                 for _ in range(num_layers)]\n#             )\n#             self.context_proj = nn.ModuleList(\n#                 [nn.Linear(context_dim, out_channels)\n#                 for _ in range(num_layers)]\n#             )\n        self.residual_input_conv = nn.ModuleList(\n            [nn.Conv2d(in_channels if i == 0 else out_channels, out_channels, kernel_size=1)\n            for i in range(num_layers)]\n        )\n        self.up_sample_conv = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, 4, 2, 1) if self.up_sample else nn.Identity()\n\n    def forward(self, x, out_down=None, t_emb=None, context=None):\n        x = self.up_sample_conv(x)\n        if out_down is not None:\n            x = torch.cat([x, out_down], dim=1)\n\n        out = x\n        for i in range(self.num_layers):\n            resnet_input = out\n            out = self.resnet_conv_first[i](out)\n            if self.t_emb_dim is not None:\n                out = out + self.t_emb_layers[i](t_emb)[:, :, None, None]\n            out = self.resnet_conv_second[i](out)\n            out = out + self.residual_input_conv[i](resnet_input)\n            batch_size, channels, h, w = out.shape\n            in_attn = out.reshape(batch_size, channels, h * w)\n            in_attn = self.attention_norms[i](in_attn)\n            in_attn = in_attn.transpose(1, 2)\n            out_attn, _ = self.attentions[i](in_attn, in_attn, in_attn)\n            out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n            out = out + out_attn\n\n            if self.cross_attn:\n                assert context is not None, \"context cannot be None if cross attention layers are used\"\n                batch_size, channels, h, w = out.shape\n                in_attn = out.reshape(batch_size, channels, h * w)\n                in_attn = self.cross_attention_norms[i](in_attn)\n                in_attn = in_attn.transpose(1, 2)\n                assert len(context.shape) == 3, \"Context shape does not match B,_,CONTEXT_DIM\"\n                assert context.shape[0] == x.shape[0] and context.shape[-1] == self.context_dim,\"Context shape does not match B,_,CONTEXT_DIM\"\n                context_proj = self.context_proj[i](context)\n                out_attn, _ = self.cross_attentions[i](in_attn, context_proj, context_proj)\n                out_attn = out_attn.transpose(1, 2).reshape(batch_size, channels, h, w)\n                out = out + out_attn\n\n        return out","metadata":{"id":"JvpvXwf004ph","execution":{"iopub.status.busy":"2024-07-16T10:45:25.075726Z","iopub.execute_input":"2024-07-16T10:45:25.076085Z","iopub.status.idle":"2024-07-16T10:45:25.096124Z","shell.execute_reply.started":"2024-07-16T10:45:25.076057Z","shell.execute_reply":"2024-07-16T10:45:25.095327Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### **UNET**","metadata":{"id":"9LoSTq_QwyFx"}},{"cell_type":"code","source":"class Unet(nn.Module):\n    def __init__(self, im_channels, model_config):\n        super().__init__()\n        self.down_channels = model_config['down_channels']\n        self.mid_channels = model_config['mid_channels']\n        self.t_emb_dim = model_config['time_emb_dim']\n        self.down_sample = model_config['down_sample']\n        self.num_down_layers = model_config['num_down_layers']\n        self.num_mid_layers = model_config['num_mid_layers']\n        self.num_up_layers = model_config['num_up_layers']\n        self.attns = model_config['attn_down']\n        self.norm_channels = model_config['norm_channels']\n        self.num_heads = model_config['num_heads']\n        self.conv_out_channels = model_config['conv_out_channels']\n\n        assert self.mid_channels[0] == self.down_channels[-1]\n        assert self.mid_channels[-1] == self.down_channels[-2]\n        assert len(self.down_sample) == len(self.down_channels) - 1\n        assert len(self.attns) == len(self.down_channels) - 1\n\n        self.t_proj = nn.Sequential(\n            nn.Linear(self.t_emb_dim, self.t_emb_dim),\n            nn.SiLU(),\n            nn.Linear(self.t_emb_dim, self.t_emb_dim)\n        )\n        self.up_sample = list(reversed(self.down_sample))\n        self.conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size=3, padding=1)\n        self.downs = nn.ModuleList([])\n        for i in range(len(self.down_channels) - 1):\n            self.downs.append(DownBlock(self.down_channels[i], self.down_channels[i + 1], self.t_emb_dim,\n                                        down_sample = self.down_sample[i], nums_head = self.nums_head,\n                                        num_layers = self.num_down_layers,\n                                        attn = self.attns[i], norm_channels = self.norm_channels))\n        self.mids = nn.ModuleList([])\n        for i in range(len(self.mid_channels) - 1):\n            self.mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i + 1], self.t_emb_dim,\n                                        num_heads = self.num_heads, num_layers = self.num_mid_layers,\n                                        norm_channels = self.norm_channels))\n        self.ups = nn.ModuleList([])\n        for i in reversed(range(len(self.down_channels) - 1)):\n            self.ups.append(UpBlockUnet(self.down_channels[i] * 2, self.down_channels[i - 1] if i != 0 else self.conv_out_channels,\n                                        self.t_emb_dim, up_sample = self.down_sample[i], num_heads = self.num_heads,\n                                        num_layers = self.num_layers, norm_channels = self.norm_channels))\n        self.norm_out = nn.GroupNorm(self.norm_channels, self.conv_out_channels)\n        self.conv_out = nn.Conv2d(self.conv_out_channels, im_channels, kernel_size = 3, padding = 1)\n\n    def forward(self, x, t):\n        out = self.conv_in(x)\n        t_emb = get_time_embedding(torch.as_tensor(t).long(), self.t_emb_dim)\n        t_emb = self.t_proj(t_emb)\n        down_outs = []\n        for idx, down in enumerate(self.downs):\n            down_outs.append(out)\n            out = down(out, t_emb)\n        for mid in self.mids:\n            out = mid(out, t_emb)\n        for up in self.ups:\n            down_out = down_outs.pop()\n            out = up(out, down_out, t_emb)\n        out = self.norm_out(out)\n        out = nn.SiLU()(out)\n        return out","metadata":{"id":"10KdygKaw1E2","execution":{"iopub.status.busy":"2024-07-16T10:45:29.584250Z","iopub.execute_input":"2024-07-16T10:45:29.585005Z","iopub.status.idle":"2024-07-16T10:45:29.603058Z","shell.execute_reply.started":"2024-07-16T10:45:29.584971Z","shell.execute_reply":"2024-07-16T10:45:29.601987Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### **LPIPS**","metadata":{"id":"XSGSejXS_UnH"}},{"cell_type":"code","source":"from __future__ import absolute_import\nfrom collections import namedtuple\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom torch.autograd import Variable\nimport numpy as np\nimport torch.nn\nimport torchvision\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"id":"sCWmFY2h5l8S","execution":{"iopub.status.busy":"2024-07-16T10:45:33.519483Z","iopub.execute_input":"2024-07-16T10:45:33.520127Z","iopub.status.idle":"2024-07-16T10:45:33.527227Z","shell.execute_reply.started":"2024-07-16T10:45:33.520098Z","shell.execute_reply":"2024-07-16T10:45:33.526221Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def spatial_average(in_tens, keepdim = True):\n    return in_tens.mean([2, 3], keepdim = keepdim)","metadata":{"id":"w6Oezd2a_yPp","execution":{"iopub.status.busy":"2024-07-16T10:45:35.526419Z","iopub.execute_input":"2024-07-16T10:45:35.527045Z","iopub.status.idle":"2024-07-16T10:45:35.532263Z","shell.execute_reply.started":"2024-07-16T10:45:35.527016Z","shell.execute_reply":"2024-07-16T10:45:35.531291Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class vgg16(torch.nn.Module):\n    def __init__(self, requires_grad = False, pretrained = True):\n        super(vgg16, self).__init__()\n        vgg_pretrained_features = torchvision.models.vgg16(pretrained = pretrained).features\n        self.slice1 = nn.Sequential()\n        self.slice2 = nn.Sequential()\n        self.slice3 = nn.Sequential()\n        self.slice4 = nn.Sequential()\n        self.slice5 = nn.Sequential()\n        self.N_slices = 5\n        for x in range(4):\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(4, 9):\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(9, 16):\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(16, 23):\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(23, 30):\n            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, X):\n        h = self.slice1(X)\n        h_relu1_2 = h\n        h = self.slice2(h)\n        h_relu2_2 = h\n        h = self.slice3(h)\n        h_relu3_3 = h\n        h = self.slice4(h)\n        h_relu4_3 = h\n        h = self.slice5(h)\n        h_relu5_3 = h\n        vgg_outputs = namedtuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])\n        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)\n        return out\n\n","metadata":{"id":"pjW9o8TrBTZB","execution":{"iopub.status.busy":"2024-07-16T10:45:38.074690Z","iopub.execute_input":"2024-07-16T10:45:38.075331Z","iopub.status.idle":"2024-07-16T10:45:38.086662Z","shell.execute_reply.started":"2024-07-16T10:45:38.075299Z","shell.execute_reply":"2024-07-16T10:45:38.085590Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class LPIPS(nn.Module):\n    def __init__(self, net = 'vgg', version = '0.1', use_dropout = True):\n        super(LPIPS, self).__init__()\n        self.version = version\n        self.scaling_layer = ScalingLayer()\n\n        self.chns = [64, 128, 256, 512, 512]\n        self.L =  len(self.chns)\n        self.net = vgg16(pretrained = True, requires_grad = False)\n\n        self.lin0 = NetLinLayer(self.chns[0], use_dropout = use_dropout)\n        self.lin1 = NetLinLayer(self.chns[1], use_dropout = use_dropout)\n        self.lin2 = NetLinLayer(self.chns[2], use_dropout = use_dropout)\n        self.lin3 = NetLinLayer(self.chns[3], use_dropout = use_dropout)\n        self.lin4 = NetLinLayer(self.chns[4], use_dropout = use_dropout)\n        self.lins = [self.lin0, self.lin1, self.lin2, self.lin3, self.lin4]\n        self.lins = nn.ModuleList(self.lins)\n\n        import inspect\n        import os\n\n#         model_path = os.path.abspath(\n#             os.path.join(inspect.getfile(self.__init__), '..', 'weights/v%s/%s.pth' % (version, net)))\n        model_path = '/root/.cache/torch/hub/checkpoints/vgg16-397923af.pth'\n        print('Loading model from: %s' % model_path)\n        self.load_state_dict(torch.load(model_path, map_location=device), strict=False)\n\n        self.eval()\n        for param in self.parameters():\n            param.requires_grad = False\n\n\n    def forward(self, in0, in1, normalize = False):\n        if normalize:\n            in0 = 2 * in0 - 1\n            in1 = 2 * in1 - 1\n\n        in0_input, in1_input = self.scaling_layer(in0), self.scaling_layer(in1)\n\n        outs0, outs1 = self.net.forward(in0_input), self.net.forward(in1_input)\n        feats0, feats1, diffs = {}, {}, {}\n\n        for kk in range(self.L):\n            feats0[kk], feats1[kk] = nn.functional.normalize(outs0[kk], dim = 1), nn.functional.normalize(outs1[kk])\n            diffs[kk] = (feats0[kk] - feats1[kk]) ** 2\n\n        res = [spatial_average(self.lins[kk](diffs[kk]), keepdim = True) for kk in range(self.L)]\n        val = 0\n\n        for l in range(self.L):\n            val += res[l]\n        return val\n\n\n","metadata":{"id":"wZVV8ufThGdw","execution":{"iopub.status.busy":"2024-07-16T10:45:43.457163Z","iopub.execute_input":"2024-07-16T10:45:43.457981Z","iopub.status.idle":"2024-07-16T10:45:43.471453Z","shell.execute_reply.started":"2024-07-16T10:45:43.457950Z","shell.execute_reply":"2024-07-16T10:45:43.470515Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class ScalingLayer(nn.Module):\n    def __init__(self):\n        super(ScalingLayer, self).__init__()\n        self.register_buffer('shift', torch.Tensor([-.030, -.088, -.188])[None, :, None, None])\n        self.register_buffer('scale', torch.Tensor([.458, .448, .450])[None, :, None, None])\n\n    def forward(self, inp):\n        return (inp - self.shift) / self.scale\n","metadata":{"id":"5xhek74aptEb","execution":{"iopub.status.busy":"2024-07-16T10:45:47.255118Z","iopub.execute_input":"2024-07-16T10:45:47.255837Z","iopub.status.idle":"2024-07-16T10:45:47.261715Z","shell.execute_reply.started":"2024-07-16T10:45:47.255803Z","shell.execute_reply":"2024-07-16T10:45:47.260772Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class NetLinLayer(nn.Module):\n    def __init__(self, chn_in, chn_out = 1, use_dropout = False):\n        super(NetLinLayer, self).__init__()\n\n        layers = [nn.Dropout(), ] if (use_dropout) else []\n        layers += [nn.Conv2d(chn_in, chn_out, 1, stride = 1, padding = 0, bias = False), ]\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.model(x)\n        return out","metadata":{"id":"4wNSwaoGpojP","execution":{"iopub.status.busy":"2024-07-16T10:45:49.255698Z","iopub.execute_input":"2024-07-16T10:45:49.256334Z","iopub.status.idle":"2024-07-16T10:45:49.262204Z","shell.execute_reply.started":"2024-07-16T10:45:49.256302Z","shell.execute_reply":"2024-07-16T10:45:49.261356Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### **Discriminator**","metadata":{"id":"HF8KYpzPuOID"}},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, im_channels = 3, conv_channels = [64, 128, 256], kernels = [4, 4, 4, 4],\n                 strides = [2, 2, 2, 1], padding = [1, 1, 1, 1]):\n        super().__init__()\n        self.im_channels = im_channels\n        activation = nn.LeakyReLU(0.2)\n        layers_dim = [self.im_channels] + conv_channels + [1]\n        self.layers = nn.ModuleList([\n            nn.Sequential(\n                nn.Conv2d(layers_dim[i], layers_dim[i + 1], kernel_size = kernels[i], stride = strides[i], padding = padding[i], bias = False if (i != 0) else True),\n                nn.BatchNorm2d(layers_dim[i + 1]) if i!= len(layers_dim) - 2 and i != 0 else nn.Identity(), activation if i != len(layers_dim) - 2 else nn.Identity())\n            for i in range(len(layers_dim) - 1)\n        ])\n\n    def forward(self, x):\n        out = x\n        for layer in self.layers:\n            out = layer(out)\n            return out","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":128},"id":"n4yjoc71uNn5","outputId":"2637f8dd-f6f5-4047-e9e2-20e3297e69b4","execution":{"iopub.status.busy":"2024-07-16T10:45:51.612791Z","iopub.execute_input":"2024-07-16T10:45:51.613431Z","iopub.status.idle":"2024-07-16T10:45:51.622174Z","shell.execute_reply.started":"2024-07-16T10:45:51.613399Z","shell.execute_reply":"2024-07-16T10:45:51.621261Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    x = torch.randn((2,3, 256, 256))\n    prob = Discriminator(im_channels=3)(x)\n    print(prob.shape)","metadata":{"id":"cN9KqMTGvvdz","execution":{"iopub.status.busy":"2024-07-16T10:45:53.852501Z","iopub.execute_input":"2024-07-16T10:45:53.852833Z","iopub.status.idle":"2024-07-16T10:45:53.980814Z","shell.execute_reply.started":"2024-07-16T10:45:53.852809Z","shell.execute_reply":"2024-07-16T10:45:53.979856Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"torch.Size([2, 64, 128, 128])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **VQVAE**","metadata":{"id":"D2BI6a3t1sTG"}},{"cell_type":"code","source":"class VQVAE(nn.Module):\n    def __init__(self, im_channels, model_config):\n        super().__init__()\n        self.down_channels = model_config['down_channels']\n        self.mid_channels = model_config['mid_channels']\n        self.down_sample = model_config['down_sample']\n        self.num_down_layers = model_config['num_down_layers']\n        self.num_mid_layers = model_config['num_mid_layers']\n        self.num_up_layers = model_config['num_up_layers']\n        self.norm_channels = model_config['norm_channels']\n\n        self.attns = model_config['attn_down']\n        self.num_heads = model_config['num_heads']\n\n        self.z_channels = model_config['z_channels']\n        self.codebook_size = model_config['codebook_size']\n        \n        assert self.mid_channels[0] == self.down_channels[-1]\n        assert self.mid_channels[-1] == self.down_channels[-1]\n        assert len(self.down_sample) == len(self.down_channels) - 1\n        assert len(self.attns) == len(self.down_channels) - 1\n        \n        \n        self.up_sample = list(reversed(self.down_sample))\n\n        self.encoder_conv_in = nn.Conv2d(im_channels, self.down_channels[0], kernel_size = 3, padding = (1, 1))\n\n        self.encoder_downs = nn.ModuleList([])\n        for i in range(len(self.down_channels) - 1):\n            self.encoder_downs.append(DownBlock(self.down_channels[i], self.down_channels[i + 1],\n                                                t_emb_dim = None, down_sample = self.down_sample[i],\n                                                num_heads = self.num_heads, num_layers = self.num_down_layers,\n                                                attn = self.attns[i], norm_channels = self.norm_channels))\n        self.encoder_mids = nn.ModuleList([])\n        for i in range(len(self.mid_channels) - 1):\n            self.encoder_mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i + 1],\n                                                t_emb_dim = None, num_heads = self.num_heads,\n                                                num_layers = self.num_mid_layers, norm_channels = self.norm_channels))\n        self.encoder_norm_out = nn.GroupNorm(self.norm_channels, self.down_channels[-1])\n        self.encoder_conv_out = nn.Conv2d(self.down_channels[-1], self.z_channels, kernel_size = 3, padding = 1)\n\n        self.pre_quant_conv = nn.Conv2d(self.z_channels, self.z_channels, kernel_size = 1)\n\n        self.embedding = nn.Embedding(self.codebook_size, self.z_channels)\n\n        self.post_quant_conv = nn.Conv2d(self.z_channels, self.z_channels, kernel_size = 1)\n        self.decoder_conv_in = nn.Conv2d(self.z_channels, self.mid_channels[-1], kernel_size = 3, padding = (1, 1))\n\n        self.decoder_mids = nn.ModuleList([])\n        for i in reversed(range(1, len(self.mid_channels))):\n            self.decoder_mids.append(MidBlock(self.mid_channels[i], self.mid_channels[i - 1], t_emb_dim = None,\n                                              num_heads = self.num_heads, num_layers = self.num_mid_layers,\n                                              norm_channels = self.norm_channels))\n        self.decoder_ups = nn.ModuleList([])\n        for i in reversed(range(1, len(self.down_channels))):\n            self.decoder_ups.append(UpBlock(self.down_channels[i], self.down_channels[i - 1], t_emb_dim = None,\n                                            up_sample = self.down_sample[i - 1], num_heads = self.num_heads,\n                                            num_layers = self.num_up_layers, attn = self.attns[i - 1],\n                                            norm_channels = self.norm_channels))\n\n        self.decoder_norm_out = nn.GroupNorm(self.norm_channels, self.down_channels[0])\n        self.decoder_conv_out = nn.Conv2d(self.down_channels[0], im_channels, kernel_size = 3, padding = (1, 1))\n\n    def quantize(self, x):\n        B, C, H, W = x.shape\n        x = x.permute(0, 2, 3, 1)\n        x = x.reshape(x.size(0), -1, x.size(-1))\n\n        dist = torch.cdist(x, self.embedding.weight[None, :].repeat((x.size(0), 1, 1)))\n        min_encoding_indices = torch.argmin(dist, dim = -1)\n\n        quant_out = torch.index_select(self.embedding.weight,0, min_encoding_indices.view(-1))\n\n        x = x.reshape((-1, x.size(-1)))\n        commitment_loss = torch.mean((quant_out.detach() - x) ** 2)\n        codebook_loss = torch.mean((quant_out - x.detach()) ** 2)\n        quantize_losses = {\n            'codebook_loss': codebook_loss,\n            'commitment_loss': commitment_loss,\n        }\n        quant_out = x + (quant_out - x).detach()\n\n        quant_out = quant_out.reshape((B, H, W, C)).permute(0, 3, 1, 2)\n        min_encoding_indices = min_encoding_indices.reshape((-1, quant_out.size(-2), quant_out.size(-1)))\n\n        return quant_out, quantize_losses\n    def encode(self, x):\n        out = self.encoder_conv_in(x)\n        for idx, down in enumerate(self.encoder_downs):\n            out = down(out)\n        for mid in self.encoder_mids:\n            out = mid(out)\n        out = self.encoder_norm_out(out)\n        out = nn.SiLU()(out)\n        out = self.encoder_conv_out(out)\n        out = self.pre_quant_conv(out)\n        out, quant_losses = self.quantize(out)\n        return out, quant_losses\n\n    def decode(self, z):\n        out = z\n        out = self.post_quant_conv(out)\n        out = self.decoder_conv_in(out)\n        for mid in self.decoder_mids:\n            out = mid(out)\n        for idx, up in enumerate(self.decoder_ups):\n            out = up(out)\n        out = self.decoder_norm_out(out)\n        out = nn.SiLU()(out)\n        out = self.decoder_conv_out(out)\n        return out\n\n    def forward(self, x):\n        z, quant_losses = self.encode(x)\n        out = self.decode(z)\n        return out, z, quant_losses","metadata":{"id":"4EMTlnH_wKP_","execution":{"iopub.status.busy":"2024-07-16T10:45:56.626976Z","iopub.execute_input":"2024-07-16T10:45:56.627377Z","iopub.status.idle":"2024-07-16T10:45:56.655530Z","shell.execute_reply.started":"2024-07-16T10:45:56.627345Z","shell.execute_reply":"2024-07-16T10:45:56.654630Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### **Linear Noise Scheduler**","metadata":{"id":"ZXDCyr49RvXs"}},{"cell_type":"code","source":"class LinearNoiseScheduler:\n    def __init__(self, num_timesteps, beta_start, beta_end):\n        self.num_timesteps = num_timesteps\n        self.beta_start = beta_start\n        self.beta_end = beta_end\n\n        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n        self.alphas = 1. - self.betas\n        self.alpha_cum_prod = torch.cumprod(self.alphas, dim = 0).to(device)\n        self.sqrt_alpha_cum_prod = torch.sqrt(self.alpha_cum_prod).to(device)\n        self.one_minus_alpha_cum_prod = torch.sqrt(1. - self.alpha_cum_prod).to(device)\n        self.sqrt_one_minus_alpha_cum_prod = torch.sqrt(1 - self.alpha_cum_prod).to(device)\n\n    def add_noise(self, original, noise, t):\n        original_shape = original.shape\n        batch_size = original_shape[0]\n\n        sqrt_alpha_cum_prod = self.sqrt_alpha_cum_prod[t].reshape(batch_size)\n        sqrt_one_minus_alpha_cum_prod = self.sqrt_one_minus_alpha_cum_prod[t].reshape(batch_size)\n\n        for _ in range(len(original_shape) - 1):\n            sqrt_alpha_cum_prod = sqrt_alpha_cum_prod.unsqueeze(-1)\n            sqrt_one_minus_alpha_cum_prod = sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n\n        return sqrt_alpha_cum_prod * original + sqrt_one_minus_alpha_cum_prod * noise\n\n    def sample_prev_timestep(self, xt, noise_pred, t):\n        x0 = (xt - (self.sqrt_one_minus_alpha_cum_prod[t] * noise_pred)) / self.sqrt_alpha_cum_prod[t]\n        x0 = torch.clamp(x0, -1., 1.)\n        mean = xt - ((self.betas[t] * noise_pred) / (self.sqrt_one_minus_alpha_cum_prod[t]))\n        mean = mean / torch.sqrt(self.alphas[t])\n\n        if t == 0:\n            return mean, x0\n        else:\n            variance = (1 - self.alpha_cum_prod[t - 1]) / (1. - self.alpha_cum_prod[t])\n            variance = variance * self.betas[t]\n            sigma = variance ** 0.5\n            z = torch.randn(xt.shape).to(xt.device)\n            return mean + sigma * z, x0","metadata":{"id":"P5pFqvoiRy1h","execution":{"iopub.status.busy":"2024-07-16T10:46:00.550572Z","iopub.execute_input":"2024-07-16T10:46:00.550918Z","iopub.status.idle":"2024-07-16T10:46:00.562783Z","shell.execute_reply.started":"2024-07-16T10:46:00.550891Z","shell.execute_reply":"2024-07-16T10:46:00.561863Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"### **Train VQVAE**","metadata":{"id":"ZgZLwY1_Zu5c"}},{"cell_type":"code","source":"import yaml\nimport argparse\nimport random\nimport torchvision\nimport os\nimport numpy as np\nfrom tqdm import tqdm\nfrom torch.optim import Adam\nfrom torchvision.utils import make_grid\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","metadata":{"id":"f5iLlNlHRz7p","execution":{"iopub.status.busy":"2024-07-16T10:46:03.184050Z","iopub.execute_input":"2024-07-16T10:46:03.185009Z","iopub.status.idle":"2024-07-16T10:46:03.190265Z","shell.execute_reply.started":"2024-07-16T10:46:03.184975Z","shell.execute_reply":"2024-07-16T10:46:03.189296Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data.dataloader import DataLoader\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-07-16T10:46:06.089870Z","iopub.execute_input":"2024-07-16T10:46:06.090504Z","iopub.status.idle":"2024-07-16T10:46:06.096173Z","shell.execute_reply.started":"2024-07-16T10:46:06.090473Z","shell.execute_reply":"2024-07-16T10:46:06.095283Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"os.mkdir('weights')\ndirectory = 'v0.1'\npath = os.path.join('/kaggle/working/weights', directory)\nos.mkdir(path)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T10:46:09.064685Z","iopub.execute_input":"2024-07-16T10:46:09.065636Z","iopub.status.idle":"2024-07-16T10:46:09.070100Z","shell.execute_reply.started":"2024-07-16T10:46:09.065601Z","shell.execute_reply":"2024-07-16T10:46:09.069305Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"image_list = []\nfor filename in glob.glob('/kaggle/input/celebritydataset/celeba_hq_256/*.jpg'): #assuming jpg \n    im = Image.open(filename) \n    image_list.append(im) ","metadata":{"execution":{"iopub.status.busy":"2024-07-16T10:46:10.838999Z","iopub.execute_input":"2024-07-16T10:46:10.839358Z","iopub.status.idle":"2024-07-16T10:47:49.525477Z","shell.execute_reply.started":"2024-07-16T10:46:10.839329Z","shell.execute_reply":"2024-07-16T10:47:49.524629Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"print(image_list[0].size)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T10:50:06.617375Z","iopub.execute_input":"2024-07-16T10:50:06.618492Z","iopub.status.idle":"2024-07-16T10:50:06.623113Z","shell.execute_reply.started":"2024-07-16T10:50:06.618437Z","shell.execute_reply":"2024-07-16T10:50:06.622269Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"(256, 256)\n","output_type":"stream"}]},{"cell_type":"code","source":"class MyDataset(torch.utils.data.Dataset):\n    def __init__(self, img_list, augmentations):\n        super(MyDataset, self).__init__()\n        self.img_list = img_list\n        self.augmentations = augmentations\n\n    def __len__(self):\n        return len(self.img_list)\n\n    def __getitem__(self, idx):\n        img = self.img_list[idx]\n        return self.augmentations(img)\n  \n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T10:50:08.481802Z","iopub.execute_input":"2024-07-16T10:50:08.482159Z","iopub.status.idle":"2024-07-16T10:50:08.488200Z","shell.execute_reply.started":"2024-07-16T10:50:08.482130Z","shell.execute_reply":"2024-07-16T10:50:08.487311Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Creating a custom dataset class \nclass ImageDataset(torch.utils.data.Dataset): \n    def __init__(self, img_list, transform): \n        self.img_list = img_list\n        self.transform = transform \n        self.counter = 0\n  \n    # Defining the length of the dataset \n    def __len__(self): \n        return len(self.img_list) \n  \n    # Defining the method to get an item from the dataset \n    def __getitem__(self, index): \n        image = self.img_list[index]\n        image = self.transform(image) \n          \n        return image","metadata":{"execution":{"iopub.status.busy":"2024-07-16T10:50:11.419372Z","iopub.execute_input":"2024-07-16T10:50:11.420154Z","iopub.status.idle":"2024-07-16T10:50:11.426303Z","shell.execute_reply.started":"2024-07-16T10:50:11.420121Z","shell.execute_reply":"2024-07-16T10:50:11.425140Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([F.to_tensor])\ndataset = ImageDataset(image_list, transform)\ndataset_length = len(dataset)\nprint(dataset_length)\ndataloader = DataLoader(dataset, batch_size = 1, shuffle = True)\nprint(len(dataloader))\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T10:50:14.691688Z","iopub.execute_input":"2024-07-16T10:50:14.692045Z","iopub.status.idle":"2024-07-16T10:50:14.698042Z","shell.execute_reply.started":"2024-07-16T10:50:14.692016Z","shell.execute_reply":"2024-07-16T10:50:14.697156Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"30000\n30000\n","output_type":"stream"}]},{"cell_type":"code","source":"def train(args):\n    with open(args.config_path, 'r') as file:\n        try:\n            config = yaml.safe_load(file)\n            print('Loaded YAML file')\n        except yaml.YAMLError as exc:\n            print(exc)\n    # print(config)\n\n    dataset_config = config['dataset_params']\n    autoencoder_config = config['autoencoder_params']\n    train_config = config['train_params']\n\n    seed = train_config['seed']\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    if device == 'cuda':\n        torch.cuda.manual_seed_all(seed)\n    model = VQVAE(im_channels = dataset_config['im_channels'],\n                  model_config = autoencoder_config).to(device)\n    \n#     im_dataset_cls = {\n#         'celebhq': CelebDataset,\n#     }.get(dataset_config['name'])\n#     print(\"im_dataset_cls = \", im_dataset_cls)\n#     im_dataset = im_dataset_cls(split = 'train', im_path = dataset_config['im_path'],\n#                                 im_size = dataset_config['im_size'],\n#                                 im_channels = dataset_config['im_channels'])\n#     print(\"Length of dataset = \",len(im_dataset))\n#     data_loader = DataLoader(im_dataset, batch_size = train_config['autoencoder_batch_size'], shuffle = True)\n    \n#     data_root = '/kaggle/input/celebritydataset'\n#     dataset = ImageFolder(data_root)\n#     print(\"datasettype = \", type(dataset[0]))\n#     print(\"datasetshape = \", len(dataset))\n#     batch_size = train_config['autoencoder_batch_size']\n#     data_loader = DataLoader(dataset = dataset, batch_size = batch_size, shuffle = True)\n#     print(\"length of dataset = \", len(dataset))\n#     for i in data_loader:\n#         print(i)\n#     transform = transforms.Compose([transforms.ToTensor()])\n#     transformed_dataset = MyDataset(dataset, transform)\n#     transformed_dataset = DataLoader(transformed_dataset, batch_size, shuffle=True)\n#     print(\"Transformed dataset shape = \", transformed_dataset.shape)\n\n    \n    if not os.path.exists(train_config['task_name']):\n        os.mkdir(train_config['task_name'])\n\n    num_epochs = train_config['autoencoder_epochs']\n\n    recon_criterion = torch.nn.MSELoss()\n    disc_criterion = torch.nn.MSELoss()\n\n    lpips_model = LPIPS().eval().to(device)\n    discriminator = Discriminator(im_channels = dataset_config['im_channels']).to(device)\n\n    optimizer_d = Adam(discriminator.parameters(), lr = train_config['autoencoder_lr'], betas = (0.5, 0.999))\n    optimizer_g = Adam(model.parameters(), lr = train_config['autoencoder_lr'], betas = (0.5, 0.999))\n\n    disc_step_start = train_config['disc_start']\n    step_count = 0\n\n    acc_steps= train_config['autoencoder_acc_steps']\n    image_save_steps = train_config['autoencoder_img_save_steps']\n    image_save_count = 0\n    print(\"Starting training\")\n    for epoch_idx in range(5):\n        recon_losses = []\n        codebook_losses = []\n        perceptual_losses = []\n        disc_losses = []\n        gen_losses = []\n        losses = []\n\n        optimizer_g.zero_grad()\n        optimizer_d.zero_grad()\n        \n        for im in tqdm(dataloader):\n            step_count += 1\n#             print(type(im))\n#             im = im.to(device)\n#             print(im)\n#             print(\"imsize = \", len(im))\n#             im = np.array(im)\n#             print(im.shape)\n#             im = torchvision.transforms.functional.pil_to_tensor(im)\n#             print(im.size())\n            \n#             transform = transforms.Compose([transforms.ToTensor()])\n#             x = transform(im)  \n            im = im.to(device)\n\n                \n#             print(type(im[0]))\n#             im = torch.stack(im, dim=0)\n#             im = np.array(im)\n#             im = im.to(device)\n#             print(im.size())\n#             transform = transforms.Compose([transforms.ToTensor()])\n#             x = transform(im)\n            model_output = model(im)\n            output, z, quantize_losses = model_output\n            \n            if step_count % image_save_steps == 0 or step_count == 1:\n                sample_size = min(8, im.shape[0])\n                save_output = torch.clamp(output[:sample_size], -1., 1.).detach().cpu()\n                save_output = ((save_output + 1) / 2)\n                save_input = ((im[:sample_size] + 1) / 2).detach().cpu()\n\n                grid = make_grid(torch.cat([save_input, save_output], dim = 0), nrow = sample_size)\n                img = torchvision.transforms.ToPILImage()(grid)\n                if not os.path.exists(os.path.join(train_config['task_name'], 'vqvae_autoencoder_samples')):\n                    os.mkdir(os.path.join(train_config['task_name'], 'vqvae_autoencoder_samples'))\n                img.save(os.path.join(train_config['task_name'], 'vqvae_autoencoder_samples', 'current_autoencoder_samples{}.png'.format(image_save_count)))\n                image_save_count += 1\n                img.close()\n\n            recon_loss = recon_criterion(output, im)\n            recon_losses.append(recon_loss.item())\n            recon_loss = recon_loss / acc_steps\n            g_loss = (recon_loss +\n                        (train_config['codebook_weight'] * quantize_losses['codebook_loss'] / acc_steps) +\n                        (train_config['commitment_beta'] * quantize_losses['commitment_loss'] / acc_steps))\n            codebook_losses.append(train_config['codebook_weight'] * quantize_losses['codebook_loss'].item())\n\n            if step_count > disc_step_start:\n                disc_fake_pred = discriminator(model_output[0])\n                disc_fake_loss = disc_criterion(disc_fake_pred, torch.ones(disc_fake_pred.shape, device = disc_fake_pred.device))\n                gen_losses.append(train_config['disc_weight'] * disc_fake_loss.item())\n                g_loss += train_config['disc_weight'] * disc_fake_loss / acc_steps;\n            lpips_loss = torch.mean(lpips_model(output, im)) / acc_steps\n            perceptual_losses.append(train_config['perceptual_weight'] * lpips_loss.item())\n            g_loss += train_config['perceptual_weight'] * lpips_loss / acc_steps\n            losses.append(g_loss.item())\n            g_loss.backward()\n\n            if step_count > disc_step_start:\n                fake = output\n                disc_fake_pred = discriminator(fake.detach())\n                disc_real_pred = discriminator(im)\n                disc_fake_loss = disc_criterion(disc_fake_pred, torch.zeros(disc_fake_pred.shape, device = disc_fake_pred.device))\n                disc_real_loss = disc_criterion(disc_real_pred, torch.ones(disc_real_pred.shape, device = disc_real_pred.device))\n                disc_loss = train_config['disc_weight'] * (disc_fake_loss + disc_real_loss) / 2\n                disc_losses.append(disc_loss.item())\n                disc_loss = disc_loss / acc_steps\n                disc_loss.backward()\n                if step_count % acc_steps == 0:\n                    optimizer_d.step()\n                    optimizer_d.zero_grad()\n\n            if step_count % acc_steps == 0:\n                optimizer_g.step()\n                optimizer_g.zero_grad()\n\n        optimizer_d.step()\n        optimizer_d.zero_grad()\n        optimizer_g.step()\n        optimizer_g.zero_grad()\n        if len(disc_losses) > 0:\n            print(\n                'Finished epoch: {} | Recon loss: {:.4f} | Perceptual loss: {:.4f}'\n                'Codebook: {:.4f} | G Loss: {:.4f} | D loss: {:.4f}'.\n                format(epoch_idx + 1, np.mean(recon_losses), np.mean(perceptual_losses), np.mean(codebook_losses), np.mean(gen_losses), np.mean(disc_losses))\n            )\n        else:\n            print(\n                'Finished epoch: {} | Recon loss: {:.4f} | Perceptual loss: {:.4f}'\n                'Codebook: {:.4f}'.\n                format(epoch_idx + 1, np.mean(recon_losses), np.mean(perceptual_losses), np.mean(codebook_losses))\n            )\n        torch.save(model.state_dict(), os.path.join(train_config['task_name'],\n                                                    train_config['vqvae_autoencoder_ckpt_name']))\n        torch.save(discriminator.state_dict(), os.path.join(train_config['task_name'],\n                                                            train_config['vqvae_discriminator_ckpt_name']))\n        print('Done training.........')\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description = 'Arguments for vqvae training')\n    parser.add_argument('--config', dest = 'config_path', default = '/kaggle/input/celebhq-yaml/celebhq.yaml', type = str)\n    args = parser.parse_args(args = [])\n    train(args)\n\n","metadata":{"id":"9oQS0WZlaSLR","execution":{"iopub.status.busy":"2024-07-16T12:57:26.999454Z","iopub.execute_input":"2024-07-16T12:57:27.000122Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Loaded YAML file\nLoading model from: /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\nStarting training\n","output_type":"stream"},{"name":"stderr","text":" 23%|██▎       | 6967/30000 [28:21<1:33:21,  4.11it/s]IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n 74%|███████▎  | 22065/30000 [1:30:17<32:37,  4.05it/s]  ","output_type":"stream"}]},{"cell_type":"code","source":"import argparse\nimport glob\nimport os\nimport pickle\nfrom torch.utils.data.dataloader import DataLoader\nfrom torchvision.utils import make_grid\nfrom tqdm import tqdm\n","metadata":{"id":"ZDN_tYPjGR5m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def infer(args):\n    with open(args.config_path, 'r') as file:\n        try:\n            config = yaml.safe_load(file)\n        except yaml.YAMLError as exc:\n            print(exc)\n    print(config)\n    dataset_config = config['dataset_params']\n    autoencoder_config = config['autoencoder_params']\n    train_config = config['train_params']\n    im_dataset_cls = {\n        'celebhq': CelebDataset,\n    }.get(dataset_config['name'])\n    im_dataset = im_dataset_cls(split='train',\n                                im_path=dataset_config['im_path'],\n                                im_size=dataset_config['im_size'],\n                                im_channels=dataset_config['im_channels'])\n    data_loader = DataLoader(im_dataset,\n                            batch_size=1,\n                            shuffle=False)\n\n    num_images = train_config['num_samples']\n    ngrid = train_config['num_grid_rows']\n    idxs = torch.randint(0, len(im_dataset) - 1, (num_images,))\n    ims = torch.cat([im_dataset[idx][None, :] for idx in idxs]).float()\n    ims = ims.to(device)\n    model = VQVAE(im_channels=dataset_config['im_channels'],\n                model_config=autoencoder_config).to(device)\n    model.load_state_dict(torch.load(os.path.join(train_config['task_name'], train_config['vqvae_autoencoder_ckpt_name']), map_location=device))\n    model.eval()\n    with torch.no_grad():\n        encoded_input, _ = model.encode(ims)\n        decoded_output = model.decode(encoded_output)\n        encoded_output = torch.clamp(encoded_output, -1., 1.)\n        encoded_output = (encoded_output + 1) / 2\n        decoded_output = torch.clamp(decoded_output, -1., 1.)\n        decoded_output = (decoded_output + 1) / 2\n        ims = (ims + 1) / 2\n\n        encoder_grid = make_grid(encoded_output.cpu(), nrow = ngrid)\n        decoder_grid = make_grid(decoded_output.cpu(), nrow = ngrid)\n        input_grid = make_grids(ims.cpu(), nrow = ngrid)\n        encoder_grid = torchvision.transforms.ToPILImage()(encoder_grid)\n        decoder_grid = torchvision.transforms.ToPILImage()(decoder_grid)\n        input_grid = torchvision.transforms.ToPILImage()(input_grid)\n\n        input_grid.save(os.path.join(train_config['task_name'], 'input_samples.png'))\n        encoder_grid.save(os.path.join(train_config['task_name'], 'encoded_samples.png'))\n        decoder_grid.save(os.path.join(train_config['task_name'], 'reconstructed_samples.png'))\n\n        if train_config['save_latents']:\n            latent_path = os.path.join(train_config['task_name'], train_config['vqvae_latent_dir_name'])\n            latent_fnames = glob.glob(os.path.join(train_config['task_name'], train_config['vqvae_latent_dir_name'], '*.pkl'))\n            assert len(latent_fnames) == 0, \"Latents are already present. Delete all latent files and rerun\"\n            if not os.path.exists(latent_path):\n                os.mkdir(latent_path)\n            print('Saving latents for {}'.format(dataset_config['name']))\n            fname_latent_map = {}\n            part_count = 0\n            count = 0\n            for idx, im in enumerate(tqdm(data_loader)):\n                encoded_output, _ = model.encode(im.float().to(device))\n                fname_latent_map[im_dataset.images[idx]] = encoded_output.cpu()\n                if(count + 1) % 1000 == 0:\n                    pickle.dump(fname_latent_map, opne(os.path.join(latent_path, '{}.pkl'.format(part_count)), 'wb'))\n                    part_count += 1\n                    fname_latent_map = {}\n                count += 1\n            if len(fname_latent_map) > 0:\n                pickle.dum(fname_latent_map, open(os.path.join(latent_path, '{}.pkl'.format(part_count)), 'wb'))\n\n            print('Done saving latents')\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description = 'Arguments for vqvae inference')\n    parser.add_argument('--config', dest = 'config_path', default = 'config/mnist.yaml', type = str)\n    args = parser.parse_args()\n    infer(args)\n","metadata":{"id":"bKaLHsGGGZpy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Train VQVAE and DDPM**","metadata":{"id":"FLEIxVeYWIg0"}},{"cell_type":"code","source":"from torch.optim import Adam\n","metadata":{"id":"sHYVDZerWM9p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def trainDDPM(args):\n    with open(args.config_path, 'r') as file:\n        try:\n            config = yaml.safe_load(file)\n        except yaml.YAMLError as exc:\n            print(exc)\n    print(config)\n\n    diffusion_config = config['diffusion_params']\n    dataset_config = config['dataset_params']\n    diffusion_model_config = config['ldm_params']\n    autoencoder_model_config = config['autoencoder_params']\n    train_config = config['train_params']\n\n    scheduler = LinearNoiseScheduler(num_timesteps=diffusion_config['num_timesteps'],\n                                     beta_start=diffusion_config['beta_start'],\n                                     beta_end=diffusion_config['beta_end'])\n    im_dataset_cls = {\n        'celebhq': CelebDataset,\n    }.get(dataset_config['name'])\n\n    im_dataset = im_dataset_cls(split='train',\n                                im_path=dataset_config['im_path'],\n                                im_size=dataset_config['im_size'],\n                                im_channels=dataset_config['im_channels'],\n                                use_latents=True,\n                                latent_path=os.path.join(train_config['task_name'], train_config['vqvae_latent_dir_name'])\n                                )\n    data_loader = DataLoader(im_dataset,\n                             batch_size=train_config['ldm_batch_size'],\n                             shuffle=True)\n    model = Unet(im_channels=autoencoder_model_config['z_channels'],\n                 model_config=diffusion_model_config).to(device)\n    model.train()\n\n    if not im_dataset.use_latents:\n        print('Loading vqvae model as latents not present')\n        vae = VQVAE(im_channels=dataset_config['im_channels'],\n                    model_config=autoencoder_model_config).to(device)\n        vae.eval()\n    if os.path.exists(os.path.join(train_config['task_name'], train_config['vqvae_autoencoder_ckpt_name'])):\n            print('Loaded vae checkpoint')\n            vae.load_state_dict(torch.load(os.path.join(train_config['task_name'], train_config['vqvae_autoencoder_ckpt_name']), map_location=device))\n\n    num_epochs = train_config['ldm_epochs']\n    optimizer = Adam(model.parameters(), lr = train_config['ldm_lr'])\n    criterion = nn.MSELoss()\n\n    if not im_dataset.use_latents:\n        for param in vae.parameters():\n            param.requires_grad = False\n\n    for epoch_idx in range(num_epochs):\n        losses = []\n        for im in tqdm(data_loader):\n            optimizer.zero_grad()\n            im = im.float().to(device)\n            if not im_dataset.use_latents:\n                with torch.no_grad():\n                    im, _ = vae.encode(im)\n\n            noise = torch.randn_like(im).to(device)\n            t = torch.randint(0, diffusion_config['num_timesteps'], (im.shape[0],)).to(device)\n            noisy_im = scheduler.add_noise(im, noise, t)\n            noise_pred = model(noisy_im, t)\n\n            loss = criterion(noise_pred, noise)\n            losses.append(loss.item())\n            loss.backward()\n            optimizer.step()\n        print('Finished epoch: {} | Loss: {:.4f}'.format{epoch_idx + 1, np.mean(losses)})\n        torch.save(model.state_dict(), os.path.join(train_config['task_name'], train_config['ldm_ckpt_name']))\n    print(\"Done training\")","metadata":{"id":"Fz0UHVYCWU1r","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Sample VQVAE and DDPM**","metadata":{"id":"Vei9XYYjY5t8"}},{"cell_type":"code","source":"def sample(model, scheduler, train_config, diffusion_model_config, autoencoder_model_config, diffusion_config, dataset_config, vae):\n    im_size = dataset_config['im_size'] // 2 ** sum(autoencoder_model_config['down_sample'])\n    xt = torch.randn((train_config['num_samples'],\n                      autoencoder_model_config['z_channels'],\n                      im_size, im_size)).to(device)\n    save_count = 0\n    for i in tqdm(reversed(range(diffusion_config['num_timesteps']))):\n        noise_pred = model(xt, torch.as_tensor(i).unsqeeze(0).to(device))\n\n        xt, x0_pred = scheduler(sample_prev_timestep(xt, noise_pred, torch.as_tensor(i).to(device)))\n        if i == 0:\n            ims = vae.decode(xt)\n        else:\n            ims = xt\n\n        ims = torch.clamp(ims, -1., 1.).detach().cpu()\n        ims = (ims + 1) / 2\n        grid = make_grid(ims, nrow = train_config['num_grid_rows'])\n        img = torchvision.transforms.ToPILImage()(grid)\n\n        if not os.path.exists(os.path.join(train_config['task_name'], 'samples')):\n            os.mkdir(os.path.join(train_config['task_name'], 'samples'))\n        img.save(os.path.join(train_config['task_name'], 'samples', 'x0_{}.png'.format(i)))\n        img.close()\n\n\n    def infer(args):\n        with open(args.config_path, 'r') as file:\n            try:\n                config = yaml.safe_load(file)\n            except yaml.YAMLError as exc:\n                print(exc)\n        print(config)\n        diffusion_config = config['diffusion_params']\n        dataset_config = config['dataset_params']\n        diffusion_model_config = config['ldm_params']\n        autoencoder_model_config = config['autoencoder_params']\n        train_config = config['train_params']\n        scheduler = LinearNoiseScheduler(num_timesteps=diffusion_config['num_timesteps'],\n                                     beta_start=diffusion_config['beta_start'],\n                                     beta_end=diffusion_config['beta_end'])\n\n        model = Unet(im_channels=autoencoder_model_config['z_channels'],\n                    model_config=diffusion_model_config).to(device)\n        model.eval()\n        if os.path.exists(os.path.join(train_config['task_name'],\n                                    train_config['ldm_ckpt_name'])):\n            print('Loaded unet checkpoint')\n            model.load_state_dict(torch.load(os.path.join(train_config['task_name'],\n                                                        train_config['ldm_ckpt_name']),\n                                            map_location=device))\n        # Create output directories\n        if not os.path.exists(train_config['task_name']):\n            os.mkdir(train_config['task_name'])\n\n        vae = VQVAE(im_channels=dataset_config['im_channels'],\n                    model_config=autoencoder_model_config).to(device)\n        vae.eval()\n        if os.path.exists(os.path.join(train_config['task_name'],\n                                                    train_config['vqvae_autoencoder_ckpt_name'])):\n            print('Loaded vae checkpoint')\n            vae.load_state_dict(torch.load(os.path.join(train_config['task_name'],\n                                                        train_config['vqvae_autoencoder_ckpt_name']),\n                                        map_location=device), strict=True)\n        with torch.no_grad():\n            sample(model, scheduler, train_config, diffusion_model_config,\n                autoencoder_model_config, diffusion_config, dataset_config, vae)\n\n","metadata":{"id":"7DqmXPC7Z0fN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Arguments for ddpm image generation')\n    parser.add_argument('--config', dest='config_path',\n                        default='config/mnist.yaml', type=str)\n    args = parser.parse_args()\n    infer(args)","metadata":{"id":"qFCgjb61lFup"},"execution_count":null,"outputs":[]}]}